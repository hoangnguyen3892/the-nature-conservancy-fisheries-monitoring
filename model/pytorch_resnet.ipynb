{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchnet.meter import AverageValueMeter, ClassErrorMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"arch\": \"resnet50\", # resnet101, resnet152, inception_v3\n",
    "    \"pretrained\": True,\n",
    "    \"datadir\": \"../data\",\n",
    "    \"cuda\": False,\n",
    "    \"optim\": \"adam\", # sgd, rmsprop\n",
    "    \"epochs\": 90,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"seed\": 7,\n",
    "    \"workers\": 4,\n",
    "    \"nb_vals\": 450,\n",
    "    \"nb_augs\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Make dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make data dir\n",
    "if not os.path.isdir(args.datadir):\n",
    "    os.makedirs(args.datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "# Seed for cuda\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train/test folders: put the data provided by Kaggle in datadir\n",
    "traindir_full = os.path.join(args.datadir, \"train\")\n",
    "testdir = os.path.join(args.datadir, \"test_stg1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# intermediate folder: this folder contains train/val/test/submit folders\n",
    "intermediate_path = os.path.join(\"..\", \"intermediate\")\n",
    "\n",
    "# train/val/test/submit folders\n",
    "traindir = os.path.join(intermediate_path, \"train\" + str(args.nb_vals))\n",
    "valdir = os.path.join(intermediate_path, \"val\" + str(args.nb_vals))\n",
    "submission_path = os.path.join(intermediate_path, \"submissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../intermediate/val450\n"
     ]
    }
   ],
   "source": [
    "print(valdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# best model path\n",
    "model_best_filename = \"model_best_{0}vals_{1}augs_{2}.pth.tar\".format(args.nb_vals, args.nb_augs, args.arch)\n",
    "model_best_filepath = os.path.join(intermediate_path, model_best_filename)\n",
    "\n",
    "# get classes\n",
    "classes = sorted([x.split(\"/\")[-1] for x in glob.glob(traindir_full+\"/*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#copy tree path of traindir_full to traindir\n",
    "if not os.path.isdir(traindir):\n",
    "    shutil.copytree(\"../data/train\", traindir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "#Use this to check the number of picture in folder\n",
    "print(len(glob.glob(traindir_full+ '/ALB/*')))\n",
    "print(len(glob.glob(traindir+ '/ALB/*')))\n",
    "#return 1719\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(valdir):\n",
    "    np.random.seed(args.seed)\n",
    "    g = glob.glob(traindir + \"/*/*.jpg\")\n",
    "    shuf = np.random.permutation(g) #randomly permute g\n",
    "    for i in range(args.nb_vals):\n",
    "        os.renames(shuf[i], shuf[i].replace(\"train\", \"val\")) #move the picture of i in shuf from train to val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "#Use this to check the number of picture in folder\n",
    "print(len(glob.glob(traindir_full+ '/ALB/*'))) #return 1719\n",
    "print(len(glob.glob(traindir+ '/ALB/*'))) #return 1511\n",
    "print(len(glob.glob(traindir+ '/ALB/*'))) #return 208\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make dir\n",
    "dir_list = [traindir_full, testdir, intermediate_path, traindir, valdir, submission_path, model_best_filepath]\n",
    "\n",
    "for i in dir_list:\n",
    "    if not os.path.isdir(i):\n",
    "        os.makedirs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```torch.utils.data.dataloader```\n",
    "\n",
    "Read about DataLoader [Pytorch](http://pytorch.org/docs/_modules/torch/utils/data/dataloader.html)\n",
    "\n",
    "num_workers (int, optional): how many subprocesses to use for data\n",
    "            loading. 0 means that the data will be loaded in the main process\n",
    "            (default: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```torchvision.datasets```\n",
    "\n",
    "Read about datasets [Pytorch](http://pytorch.org/docs/torchvision/datasets.html?highlight=dataset)\n",
    "\n",
    "dset.ImageFolder(root=\"root folder path\", [transform, target_transform])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```torchvision.transforms```\n",
    "\n",
    "Read about transforms [pytorch](http://pytorch.org/docs/torchvision/transforms.html?highlight=transform)\n",
    "\n",
    "class ```torchvision.transforms.Compose(transforms)```: Composes several transforms together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load train\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.ImageFolder(traindir,\n",
    "                         transforms.Compose([\n",
    "                             transforms.Scale(400),\n",
    "                             transforms.RandomSizedCrop(224),\n",
    "                             transforms.RandomHorizontalFlip(),\n",
    "                             transforms.ToTensor(),\n",
    "                             normalize])),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load val\n",
    "val_loader = DataLoader(\n",
    "    datasets.ImageFolder(valdir,\n",
    "                         transforms.Compose([\n",
    "                             transforms.Scale(400),\n",
    "                             transforms.RandomSizedCrop(224),\n",
    "                             transforms.ToTensor(),\n",
    "                             normalize])),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TestImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        images = []\n",
    "        for filepath in sorted(glob.glob(root + \"/*.jpg\")):\n",
    "            images.append(filepath.split(\"/\")[-1])\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load test\n",
    "test_loader = DataLoader(\n",
    "    TestImageFolder(testdir, \n",
    "                    transforms.Compose([\n",
    "                        transforms.Scale(400),\n",
    "                        transforms.RandomSizedCrop(224),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize])),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How to load pretrained model in Pytorch: [torchvision.models](http://pytorch.org/docs/torchvision/models.html?highlight=models)\n",
    "\n",
    "```torchvision.models.alexnet(pretrained=False, **kwargs)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using pre-trained model 'resnet50'\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "if args.pretrained:\n",
    "    print(\"=> Using pre-trained model '{}'\".format(args.arch))\n",
    "    model = models.__dict__[args.arch](pretrained=True)\n",
    "else:\n",
    "    print(\"=> Creating model '{}'\".format(args.arch))\n",
    "    model = models.__dict__[args.arch]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "for param in model.parameters():\n",
    "    print(\"Data {0} \\n Type {1} \\n Size {2}\".format(param.data, type(param.data), param.size))\n",
    "    break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# parameters of newly constructed modules have requires_grad=True by default\n",
    "# replace the last fully-connected layer\n",
    "model.fc = nn.Linear(2048, len(classes))\n",
    "# for 1 GPU, it is unnecessary to use DataParallel\n",
    "#model = torch.nn.DataParallel(model).cuda()\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if args.cuda:\n",
    "    criterion.cuda()\n",
    "\n",
    "# define optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(model.fc.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "    \n",
    "elif args.optim == \"adam\":\n",
    "    optimizer = optim.Adam(model.fc.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.weight_decay)\n",
    "    \n",
    "elif args.optim == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(model.fc.parameters(),\n",
    "                              lr=args.lr,\n",
    "                              weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    checkpoint_filepath = os.path.join(intermediate_path, filename)\n",
    "    torch.save(state, checkpoint_filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(checkpoint_filepath, model_best_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n",
    "    \"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lr': 0.001, 'weight_decay': 0.0001, 'eps': 1e-08, 'params': [Parameter containing:\n",
      " 1.3321e-03  1.2238e-02 -8.6919e-03  ...   7.1346e-03  1.6748e-03 -1.9275e-02\n",
      "-1.4806e-02 -5.3727e-03  8.5778e-03  ...   1.0864e-02 -7.9852e-03  6.8600e-03\n",
      "-1.5281e-02 -7.3596e-03  2.0292e-02  ...   3.0460e-03 -8.4428e-03  1.4721e-02\n",
      "                ...                   â‹±                   ...                \n",
      " 8.2459e-03 -1.5521e-02  7.7894e-03  ...   1.7897e-02  9.4876e-03  1.3424e-02\n",
      " 7.8000e-03 -1.4753e-03  4.3713e-03  ...  -9.5337e-03 -2.3425e-03  7.0613e-03\n",
      " 2.0202e-02  7.6480e-03 -1.7076e-02  ...   1.5685e-02  8.2914e-03  3.1621e-03\n",
      "[torch.FloatTensor of size 8x2048]\n",
      ", Parameter containing:\n",
      "1.00000e-02 *\n",
      " -1.3062\n",
      "  1.6938\n",
      " -1.3604\n",
      "  1.1787\n",
      " -0.9673\n",
      "  1.1452\n",
      " -2.0635\n",
      " -0.8837\n",
      "[torch.FloatTensor of size 8]\n",
      "], 'betas': (0.9, 0.999)}]\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.param_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define train/validate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```class AverageValueMeter(meter.Meter)```\n",
    "\n",
    "[Link](https://github.com/pytorch/tnt/blob/master/torchnet/meter/averagevaluemeter.py)\n",
    "\n",
    "- def add(self, value, n = 1)\n",
    "- def value(self) return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```class ClassErrorMeter(meter.Meter)```\n",
    "\n",
    "[Link](https://github.com/pytorch/tnt/blob/master/torchnet/meter/classerrormeter.py)\n",
    "\n",
    "- def add(self, output, target)\n",
    "- def value(self, k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(args, train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train() # turn on train mode\n",
    "    \n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True) # accuracy instead of error\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(train_loader):      \n",
    "        # here we should call cuda() for input;\n",
    "        # in the ImageNet example, the model is parallel by\n",
    "        # torch.nn.DataParallel(model).cuda(), so no need to call cuda() there;\n",
    "        # the option async=True works with pin_memory of DataLoader\n",
    "        # pin_memory slows down DataLoader but fastens data transfer from\n",
    "        # CPU to GPU\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        input_size = input.size(0)    \n",
    "        input_var = Variable(input)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.add(loss.data[0] * input_size, input_size)\n",
    "        top1.add(output.data, target)\n",
    "        \n",
    "    print(\"==> EPOCH {0} | Time: {1:.3f} | Accuracy: {2:.3f} | Loss: {3:.4f}\"\n",
    "          .format(epoch, time.time()-start,\n",
    "                  top1.value()[0], losses.value()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# validate function\n",
    "def validate(args, val_loader, model, criterion):\n",
    "    model.train(False) # turn off train mode\n",
    "    \n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True)\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if args.cuda:\n",
    "            input = input.cuda(async=True)\n",
    "            target = target.cuda(async=True)\n",
    "        \n",
    "        input_size = input.size(0)\n",
    "        input_var = Variable(input, volatile=True) # no gradient\n",
    "        target_var = Variable(target, volatile=True)\n",
    "        \n",
    "        #compute output and loss\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        losses.add(loss.data[0] * input_size, input_size)\n",
    "        top1.add(output.data, target)\n",
    "    \n",
    "    print('top1.value {}'.format(top1.value))\n",
    "    print(\"==> VALIDATE | Time: {0:.3f} | Accuracy: {1:.3f} | Loss: {2:.4f}\"\n",
    "          .format(time.time()-start, top1.value()[0], losses.value()[0]))\n",
    "    return top1.value()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting to train on 'resnet50' model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.10_1488753987449/work/torch/lib/THNN/generic/ClassNLLCriterion.c:57",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7c22cf4b2fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7afd79b46762>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# compute output and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# compute gradient and do backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoang/miniconda2/envs/pydata/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoang/miniconda2/envs/pydata/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         return F.cross_entropy(input, target,\n\u001b[0;32m--> 316\u001b[0;31m                                self.weight, self.size_average)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoang/miniconda2/envs/pydata/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \"\"\"\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoang/miniconda2/envs/pydata/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoang/miniconda2/envs/pydata/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,\n\u001b[0;32m---> 41\u001b[0;31m                                                    output, *self.additional_args)\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.10_1488753987449/work/torch/lib/THNN/generic/ClassNLLCriterion.c:57"
     ]
    }
   ],
   "source": [
    "if 1 == 1:\n",
    "    print(\"=> Starting to train on '{}' model\".format(args.arch))\n",
    "    best_prec1 = 0\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        adjust_learning_rate(args, optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(args, train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(args, val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            \"epoch\": epoch,\n",
    "            \"arch\": args.arch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"best_prec1\": best_prec1,\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test(args, test_loader, model):\n",
    "    # placeholder arrays for predictions and id column\n",
    "    preds = np.zeros(shape=(len(test_loader), len(classes)))\n",
    "    id_col = []\n",
    "    \n",
    "    # turn off train mode\n",
    "    model.train(False)\n",
    "    \n",
    "    # average predictions across several different augmentations\n",
    "    for aug in range(args.nb_augs):\n",
    "        print(\"   * Predicting on test augmentation {}\".format(aug + 1))\n",
    "        \n",
    "        # iterate through image data, one file at a time\n",
    "        # (assuming batch size set to 1)\n",
    "        for i, (input, filename) in enumerate(test_loader):\n",
    "            # batch_size = 1\n",
    "            filename = filename[0]\n",
    "                     \n",
    "            if args.cuda:\n",
    "                input = input.cuda()\n",
    "            input_var = Variable(input, volatile=True) # no gradient\n",
    "            output = model(input_var)\n",
    "            softmax = F.softmax(output)[0].data.cpu().numpy()\n",
    "            \n",
    "            # add the scaled class probabilities\n",
    "            preds[i] += softmax\n",
    "            if aug == 0:\n",
    "                id_col.append(filename)\n",
    "       \n",
    "    # convert averaged prediction array to pandas dataframe\n",
    "    preds /= args.nb_augs\n",
    "    pred = pd.DataFrame(preds, columns=[classes])\n",
    "    pred[\"image\"] = id_col\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"=> Starting to test on '{}' model\".format(args.arch))\n",
    "if os.path.isfile(model_best_filepath):\n",
    "    print(\"=> Loading checkpoint '{}'\".format(model_best_filename))\n",
    "    checkpoint = torch.load(model_best_filepath)\n",
    "    best_prec1 = checkpoint[\"best_prec1\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(\"=> Loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(model_best_filename, checkpoint[\"epoch\"]))\n",
    "    pred = test(args, test_loader, model)\n",
    "    # filename for our submission file w/ extra info about this test run\n",
    "    sub_fn = \"{0}epoches_{1}vals_{2}augs_{3}.csv\".format(\n",
    "        checkpoint[\"epoch\"], args.nb_vals, args.nb_augs, args.arch)\n",
    "    # write predictions to csv\n",
    "    pred.to_csv(os.path.join(submission_path, sub_fn), index=False)\n",
    "else:\n",
    "    print(\"=> No checkpoint found at '{}'\".format(model_best_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
